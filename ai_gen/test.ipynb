{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T12:15:42.808112Z",
     "start_time": "2025-05-14T12:15:25.164099Z"
    }
   },
   "source": [
    "# memory.py (as it would be structured)\n",
    "from __future__ import annotations  # Required if type hinting Transition in older Python\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args) -> None:\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int) -> list[Transition]:\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Your DQN.py content\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def _init_weights(layer: nn.Module, nonlinearity: str) -> None:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity=nonlinearity)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_obs: int, n_act: int, n_hidden: int = 128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.obs_dim = n_obs\n",
    "        self.act_dim = n_act\n",
    "        self.hidden_dim = n_hidden\n",
    "        # model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.obs_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.act_dim)\n",
    "        )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        # re-initialize learnable parameters\n",
    "        for layer_idx, layer in enumerate(self.model):  # Iterate with index to get nonlinearity for previous layer\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nonlinearity = 'relu'  # Default for hidden layers\n",
    "                if layer_idx == len(self.model) - 1:  # Last linear layer\n",
    "                    nonlinearity = 'linear'  # Or some other appropriate for output Q-values if not followed by ReLU\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity=nonlinearity)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "            # Note: The original _init_weights only accepted 'relu'.\n",
    "            # Kaiming normal usually wants the nonlinearity of the *current* layer if it's like Conv2d->ReLU.\n",
    "            # For Linear->ReLU->Linear->ReLU->Linear, the 'relu' makes sense for the first two.\n",
    "            # For the last linear layer, 'linear' or 'leaky_relu' (if that was used) might be more standard for kaiming_normal's `nonlinearity` param.\n",
    "            # However, using 'relu' for all as in the original simplified _reset_parameters is also a common approach.\n",
    "            # I'll stick to a slightly more robust _reset_parameters matching the original intent for `_init_weights`.\n",
    "        for model_layer in self.model:  # Simpler loop, assumes 'relu' for init if Linear\n",
    "            _init_weights(model_layer, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        res = self.model(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "# Your agent.py content (DqnAgent and DqnAgentConfig parts)\n",
    "# from __future__ import annotations # Already at the top of memory.py\n",
    "import random\n",
    "from collections import OrderedDict  # Not strictly needed for DqnAgent here, but CqlAgent used it\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Callable, Union\n",
    "\n",
    "\n",
    "# import torch # already imported\n",
    "# import torch.nn as nn # already imported\n",
    "# from memory import ReplayMemory, Transition # Defined above\n",
    "# from dqn import DQN # Defined above\n",
    "\n",
    "@dataclass\n",
    "class DqnAgentConfig:\n",
    "    # env info\n",
    "    obs_dim: int = None\n",
    "    act_dim: int = None\n",
    "    hidden_dim: int = 128\n",
    "    # training\n",
    "    batch_size: int = 128\n",
    "    lr: float = 1e-4\n",
    "    grad_clip_value: float = 100\n",
    "    # gamma: discount factor\n",
    "    gamma: float = 0.99\n",
    "    # epsilon: exploration probability\n",
    "    eps_start: float = 0.9\n",
    "    eps_decay: float = 0.995  # Pursuit might need slower decay\n",
    "    eps_min: float = 0.05\n",
    "    # replay memory\n",
    "    mem_size: int = 10_000\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        assert (self.obs_dim is not None), \"obs_dim must be set in DqnAgentConfig\"\n",
    "        assert (self.act_dim is not None), \"act_dim must be set in DqnAgentConfig\"\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# CqlAgentConfig and CqlAgent are omitted as they are not used for IQL\n",
    "\n",
    "class DqnAgent:\n",
    "    def __init__(self, sid: str, config: DqnAgentConfig, act_sampler: Callable, device=None):\n",
    "        self.sid = sid\n",
    "        self.config = config\n",
    "        self.config.validate()\n",
    "        self.act_sampler = act_sampler  # action sampler function\n",
    "        self.replay_memory = ReplayMemory(capacity=self.config.mem_size)\n",
    "        self.eps = self.config.eps_start\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else\n",
    "            'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else  # mps check\n",
    "            'cpu'\n",
    "        )\n",
    "        # DQN\n",
    "        self.policy_net = DQN(n_obs=self.config.obs_dim, n_act=self.config.act_dim, n_hidden=self.config.hidden_dim).to(\n",
    "            self.device)\n",
    "        self.target_net = DQN(n_obs=self.config.obs_dim, n_act=self.config.act_dim, n_hidden=self.config.hidden_dim).to(\n",
    "            self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        # opt & loss criterion\n",
    "        self.opt = torch.optim.AdamW(self.policy_net.parameters(), lr=self.config.lr, amsgrad=True)\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    def select_action(self, state: torch.Tensor, **kwargs):  # state is already a tensor\n",
    "        return self._select_action_eps(state, dqn=self.policy_net, **kwargs)\n",
    "\n",
    "    def select_action_greedy(self, state: torch.Tensor, dqn: nn.Module, **kwargs):  # state is already a tensor\n",
    "        return self._select_action_eps(state, dqn=dqn, eps=0, **kwargs)\n",
    "\n",
    "    def _select_action_eps(self, state: torch.Tensor, dqn: nn.Module, eps: float = -1, **kwargs):\n",
    "        \"\"\"\n",
    "        input shape: 1 x obs_dim\n",
    "        output shape: 1 x 1 (tensor containing the action index)\n",
    "        \"\"\"\n",
    "        if eps == -1:\n",
    "            eps = self.eps\n",
    "        if random.random() < eps:\n",
    "            sample_res = torch.tensor([[self.act_sampler()]], device=self.device, dtype=torch.long)\n",
    "            return sample_res\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values: torch.Tensor = dqn(state)  # state is already 1 x obs_dim\n",
    "                sel_res = q_values.argmax(dim=1).reshape(1, 1)\n",
    "                return sel_res\n",
    "\n",
    "    def train(self) -> float | None:  # Returns loss value or None\n",
    "        if len(self.replay_memory) < self.config.batch_size:\n",
    "            return None  # Indicating no training was done\n",
    "\n",
    "        transitions = self.replay_memory.sample(self.config.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=self.device,\n",
    "                                      dtype=torch.bool)\n",
    "\n",
    "        non_final_nxt_states_list = [s for s in batch.next_state if s is not None]\n",
    "        if len(non_final_nxt_states_list) > 0:\n",
    "            non_final_nxt_states = torch.cat(non_final_nxt_states_list)\n",
    "        else:\n",
    "            non_final_nxt_states = torch.empty((0, self.config.obs_dim), device=self.device, dtype=torch.float32)\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        q_values_batch: torch.Tensor = self.policy_net(state_batch)\n",
    "        state_action_q_values = q_values_batch.gather(1, action_batch)\n",
    "\n",
    "        next_state_best_q_values = torch.zeros(self.config.batch_size, device=self.device)\n",
    "        if non_final_nxt_states.nelement() > 0:\n",
    "            with torch.no_grad():\n",
    "                next_state_best_q_values[non_final_mask] = self.target_net(non_final_nxt_states).max(1).values\n",
    "\n",
    "        expected_state_action_q_values = reward_batch + (\n",
    "                    self.config.gamma * next_state_best_q_values.reshape(self.config.batch_size, 1))\n",
    "\n",
    "        loss = self.criterion(state_action_q_values, expected_state_action_q_values)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(self.policy_net.parameters(), self.config.grad_clip_value)\n",
    "        self.opt.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def memorize(self, state: torch.Tensor, action: torch.Tensor, next_state: torch.Tensor | None,\n",
    "                 reward: torch.Tensor) -> None:\n",
    "        # Assumes inputs are already correctly shaped tensors\n",
    "        self.replay_memory.push(state, action, next_state, reward)\n",
    "\n",
    "    def update_target_network(self) -> None:\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def update_eps(self):\n",
    "        self.eps = max(self.config.eps_min, self.eps * self.config.eps_decay)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Main script for IQL on Pursuit\n",
    "import gymnasium  # PettingZoo uses Gymnasium spaces\n",
    "import numpy as np\n",
    "# import torch # already imported\n",
    "# import torch.nn as nn # already imported\n",
    "# import torch.optim as optim # DqnAgent handles this\n",
    "# import random # already imported\n",
    "# from collections import deque, namedtuple # ReplayMemory handles this\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from IPython import display # Handled by is_ipython check\n",
    "from itertools import count\n",
    "from pprint import pprint\n",
    "import time  # For render delay\n",
    "\n",
    "from pettingzoo.sisl import pursuit_v4\n",
    "\n",
    "\n",
    "# --- Helper function to preprocess observations ---\n",
    "def preprocess_observation(obs_numpy: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Flattens and converts numpy observation to a tensor of shape (1, obs_dim).\"\"\"\n",
    "    flat_obs = obs_numpy.flatten().astype(np.float32)\n",
    "    return torch.tensor(flat_obs, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Environment and Agent Configuration\n",
    "MAX_CYCLES = 200\n",
    "N_PURSERS = 8\n",
    "N_EVADERS = 4  # Keep it simple for IQL demo\n",
    "ENV_RENDER_MODE = None  # \"human\" or None\n",
    "\n",
    "env = pursuit_v4.parallel_env(\n",
    "    n_pursuers=N_PURSERS,\n",
    "    n_evaders=N_EVADERS,\n",
    "    max_cycles=MAX_CYCLES,\n",
    "    # obs_type='grid', # <--- REMOVED THIS ARGUMENT\n",
    "    render_mode=ENV_RENDER_MODE\n",
    ")\n",
    "init_obs_dict, init_infos = env.reset(seed=42)  # Use a seed for reproducibility\n",
    "\n",
    "pursuer_ids = [agent_id for agent_id in env.agents if \"pursuer\" in agent_id]\n",
    "evader_ids = [agent_id for agent_id in env.agents if \"evader\" in agent_id]\n",
    "print(f\"Pursuers: {pursuer_ids}\")\n",
    "print(f\"Evaders: {evader_ids}\")\n",
    "\n",
    "# Determine obs and action dimensions for pursuers\n",
    "sample_pursuer_id = pursuer_ids[0]\n",
    "raw_obs_space_shape = env.observation_space(sample_pursuer_id).shape\n",
    "obs_dim_pursuer = np.prod(raw_obs_space_shape)  # Flattened\n",
    "act_dim_pursuer = env.action_space(sample_pursuer_id).n\n",
    "print(f\"Pursuer Obs Dim (flat): {obs_dim_pursuer}\")\n",
    "print(f\"Pursuer Act Dim: {act_dim_pursuer}\")\n",
    "\n",
    "# Create DQN agents for pursuers\n",
    "# Use a common device for all agents and tensors\n",
    "training_device = torch.device('cuda' if torch.cuda.is_available() else \\\n",
    "                                   'mps' if hasattr(torch.backends,\n",
    "                                                    'mps') and torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {training_device}\")\n",
    "\n",
    "cur_agents_pursuers = {\n",
    "    agent_id: DqnAgent(\n",
    "        sid=agent_id,\n",
    "        config=DqnAgentConfig(\n",
    "            obs_dim=obs_dim_pursuer,\n",
    "            act_dim=act_dim_pursuer,\n",
    "            hidden_dim=128,  # Can be tuned\n",
    "            batch_size=64,  # Can be tuned\n",
    "            lr=1e-4,  # Can be tuned\n",
    "            gamma=0.99,\n",
    "            eps_start=1.0,\n",
    "            eps_decay=0.999,  # Slower decay for more exploration\n",
    "            eps_min=0.05,  # Minimum exploration\n",
    "            mem_size=30000,  # Memory size\n",
    "            grad_clip_value=1.0  # Clip gradients to prevent explosion\n",
    "        ),\n",
    "        act_sampler=lambda ag_id=agent_id: env.action_space(ag_id).sample(),  # Lambda to ensure correct agent_id scope\n",
    "        device=training_device\n",
    "    )\n",
    "    for agent_id in pursuer_ids\n",
    "}\n",
    "print(f\"Initialized {len(cur_agents_pursuers)} pursuer agents.\")\n",
    "\n",
    "\n",
    "# %% md\n",
    "# ## Evaluation Function (Adapted for new DqnAgent)\n",
    "# %%\n",
    "def eval_pursuit_agents_new(\n",
    "        dqn_pursuer_agents: dict[str, DqnAgent],\n",
    "        dqns_to_use_type: str,  # \"policy\" or \"target\"\n",
    "        n_episodes: int = 1,\n",
    "        render: bool = False,\n",
    "        eval_device: torch.device = torch.device('cpu')  # Device for evaluation\n",
    ") -> dict[str, list[float]]:\n",
    "    eval_env_params = dict(\n",
    "        n_pursuers=N_PURSERS,\n",
    "        n_evaders=N_EVADERS,\n",
    "        max_cycles=MAX_CYCLES,\n",
    "        # obs_type='grid', # <--- REMOVED THIS ARGUMENT\n",
    "        render_mode=('human' if render else None)\n",
    "    )\n",
    "    eval_env = pursuit_v4.parallel_env(**eval_env_params)\n",
    "\n",
    "    cumulative_rewards_pursuers = {pursuer_id: [] for pursuer_id in dqn_pursuer_agents.keys()}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        states_dict_numpy, infos_dict = eval_env.reset(seed=42 + i)\n",
    "        current_episode_rewards_pursuers = {pursuer_id: 0.0 for pursuer_id in dqn_pursuer_agents.keys()}\n",
    "\n",
    "        # active_agents_in_env = set(eval_env.agents) # PettingZoo's env.agents updates dynamically\n",
    "\n",
    "        for t_step in count():\n",
    "            if render and eval_env.render_mode == \"human\":\n",
    "                eval_env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            actions_to_env = {}\n",
    "            # Pursuers select actions greedily\n",
    "            for pursuer_id, agent_obj in dqn_pursuer_agents.items():\n",
    "                if pursuer_id in eval_env.agents and pursuer_id in states_dict_numpy:\n",
    "                    state_tensor = preprocess_observation(states_dict_numpy[pursuer_id], eval_device)\n",
    "                    network = agent_obj.policy_net if dqns_to_use_type == \"policy\" else agent_obj.target_net\n",
    "                    action_tensor = agent_obj.select_action_greedy(state_tensor, network)\n",
    "                    actions_to_env[pursuer_id] = action_tensor.item()\n",
    "\n",
    "            # Evaders take random actions\n",
    "            for evader_id in evader_ids:  # Assuming evader_ids are fixed for the scenario\n",
    "                if evader_id in eval_env.agents and evader_id in states_dict_numpy:\n",
    "                    actions_to_env[evader_id] = eval_env.action_space(evader_id).sample()\n",
    "\n",
    "            filtered_actions_to_env = {agent_id: act for agent_id, act in actions_to_env.items() if\n",
    "                                       agent_id in eval_env.agents}\n",
    "\n",
    "            if not eval_env.agents:\n",
    "                break\n",
    "            # If filtered_actions_to_env is empty but env.agents is not, it means no actions were generated for live agents.\n",
    "            # This could happen if e.g. all pursuers are done but evaders remain.\n",
    "            # PettingZoo step expects actions only for live agents if you pass a filtered dict.\n",
    "            # If you pass actions for all original agents, it internally filters.\n",
    "            # For simplicity, we build actions for agents we know are live and active this turn.\n",
    "            if not filtered_actions_to_env and eval_env.agents:\n",
    "                # print(f\"Warning: No actions in filtered_actions_to_env but agents {eval_env.agents} are live.\")\n",
    "                # This can happen if all learning agents (pursuers) are done. Let PettingZoo handle it if needed.\n",
    "                # Or, ensure evaders always have an action if they are live.\n",
    "                for ev_id_check in evader_ids:  # Ensure evaders get random if they are the only ones left\n",
    "                    if ev_id_check in eval_env.agents and ev_id_check not in filtered_actions_to_env and ev_id_check in states_dict_numpy:\n",
    "                        filtered_actions_to_env[ev_id_check] = eval_env.action_space(ev_id_check).sample()\n",
    "\n",
    "            next_states_dict_numpy, rewards_dict, terminations_dict, truncations_dict, infos_dict = eval_env.step(\n",
    "                actions_to_env)  # Use original actions_to_env; PZ filters\n",
    "\n",
    "            for pursuer_id in dqn_pursuer_agents.keys():\n",
    "                if pursuer_id in rewards_dict:\n",
    "                    current_episode_rewards_pursuers[pursuer_id] += rewards_dict[pursuer_id]\n",
    "\n",
    "            if not eval_env.agents or t_step >= MAX_CYCLES - 1:  # Check if all agents are done or max cycles\n",
    "                break\n",
    "            states_dict_numpy = next_states_dict_numpy\n",
    "\n",
    "        for pursuer_id in dqn_pursuer_agents.keys():\n",
    "            cumulative_rewards_pursuers[pursuer_id].append(current_episode_rewards_pursuers[pursuer_id])\n",
    "\n",
    "        if render and eval_env.render_mode == \"human\": eval_env.render()\n",
    "\n",
    "    eval_env.close()\n",
    "    return cumulative_rewards_pursuers\n",
    "\n",
    "\n",
    "# get_agent_wise_cumulative_rewards (same as before)\n",
    "def get_agent_wise_cumulative_rewards(cumulative_rewards: dict[str, list[float]]) -> dict[str, float]:\n",
    "    return {\n",
    "        agent_key: sum(agent_episode_rewards) / len(agent_episode_rewards) if agent_episode_rewards else 0.0\n",
    "        for agent_key, agent_episode_rewards in cumulative_rewards.items()\n",
    "    }\n",
    "\n",
    "\n",
    "# %% md\n",
    "# Baseline Evaluation\n",
    "# %%\n",
    "print(\"Evaluating untrained agents (using policy_net, which is same as target_net initially)...\")\n",
    "baseline_eval_res = eval_pursuit_agents_new(cur_agents_pursuers, dqns_to_use_type=\"policy\", n_episodes=5,\n",
    "                                            eval_device=training_device)\n",
    "avg_baseline_res = get_agent_wise_cumulative_rewards(baseline_eval_res)\n",
    "print(\"> Average cumulative rewards (baseline) per pursuer:\")\n",
    "pprint(avg_baseline_res)\n",
    "all_avg_baseline_res = sum(avg_baseline_res.values()) / len(avg_baseline_res) if avg_baseline_res and len(\n",
    "    avg_baseline_res) > 0 else -float('inf')\n",
    "print(f\"> Overall average cumulative reward (baseline): {all_avg_baseline_res:.2f}\")\n",
    "\n",
    "# %% md\n",
    "# ## Training\n",
    "# %%\n",
    "# Plotting\n",
    "is_ipython = 'inline' in matplotlib.get_backend() if ('matplotlib' in globals() and 'plt' in globals()) else False\n",
    "if is_ipython: from IPython import display\n",
    "\n",
    "\n",
    "def plot_episodes_rewards(episode_mean_rewards_all_agents: list[float], title='Training... Pursuer Avg. Return'):\n",
    "    if not plt: return  # Matplotlib not available\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg. Return (Pursuers)')\n",
    "    rewards_t = torch.tensor(episode_mean_rewards_all_agents, dtype=torch.float)\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Avg Return')\n",
    "    \n",
    "\n",
    "    if len(rewards_t) >= 10:\n",
    "        means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "        padding_size = min(9, len(rewards_t) - 1) if len(rewards_t) > 1 else 0\n",
    "        padding = torch.full((padding_size,), rewards_t[0] if len(rewards_t) > 0 else 0.0)\n",
    "\n",
    "        plot_means = torch.cat((padding, means))\n",
    "        if len(plot_means) == len(rewards_t):\n",
    "            plt.plot(plot_means.numpy(), label='10-ep Mov Avg')\n",
    "        elif len(means) > 0:\n",
    "            plt.plot(range(len(rewards_t) - len(means), len(rewards_t)), means.numpy(), label='10-ep Mov Avg (part)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        plt.draw()\n",
    "        plt.show(block=False)\n",
    "\n",
    "\n",
    "N_TRAIN_EPISODES = 100\n",
    "TARGET_NET_UPDATE_STRATEGY_FREQ = 20  # Episodes\n",
    "\n",
    "\n",
    "def update_all_target_networks_if_better_new(\n",
    "        current_pursuer_agents: dict[str, DqnAgent],\n",
    "        current_best_mean_reward: float,\n",
    "        eval_episodes: int = 5,\n",
    "        device_for_eval: torch.device = torch.device('cpu')\n",
    ") -> float:\n",
    "    print(\"Evaluating current policy networks to consider target update...\")\n",
    "    eval_res = eval_pursuit_agents_new(current_pursuer_agents, dqns_to_use_type=\"policy\", n_episodes=eval_episodes,\n",
    "                                       eval_device=device_for_eval)\n",
    "    avg_eval_res_per_agent = get_agent_wise_cumulative_rewards(eval_res)\n",
    "    if not avg_eval_res_per_agent: return current_best_mean_reward  # No results\n",
    "\n",
    "    current_overall_mean_reward = sum(avg_eval_res_per_agent.values()) / len(avg_eval_res_per_agent)\n",
    "    print(\n",
    "        f\"  Evaluation of policy_nets: Overall mean reward = {current_overall_mean_reward:.3f} (vs best: {current_best_mean_reward:.3f})\")\n",
    "    if current_overall_mean_reward > current_best_mean_reward:\n",
    "        print(f\"  New best performance! Updating target networks for all pursuers.\")\n",
    "        for agent in current_pursuer_agents.values():\n",
    "            agent.update_target_network()\n",
    "        return current_overall_mean_reward\n",
    "    else:\n",
    "        print(f\"  No improvement. Target networks remain unchanged.\")\n",
    "        return current_best_mean_reward\n",
    "\n",
    "\n",
    "# %%\n",
    "# Main Training Loop\n",
    "print(f\"\\nStarting training for {N_TRAIN_EPISODES} episodes...\")\n",
    "current_best_overall_reward = all_avg_baseline_res if all_avg_baseline_res != -float('inf') else -float('inf')\n",
    "episode_mean_rewards_history = [all_avg_baseline_res] if all_avg_baseline_res != -float('inf') else []\n",
    "\n",
    "for i_episode in range(N_TRAIN_EPISODES):\n",
    "    states_dict_numpy, infos_dict = env.reset(seed=1000 + i_episode)\n",
    "\n",
    "    episode_total_loss_pursuers = {p_id: 0.0 for p_id in pursuer_ids}\n",
    "    episode_steps_with_loss_pursuers = {p_id: 0 for p_id in pursuer_ids}  # Track steps where loss was computed\n",
    "\n",
    "    for t_step in count():\n",
    "        actions_to_env = {}\n",
    "        current_pursuer_actions_for_memory = {}\n",
    "\n",
    "        active_pursuers_this_step = [pid for pid in pursuer_ids if pid in env.agents and pid in states_dict_numpy]\n",
    "        for pursuer_id in active_pursuers_this_step:\n",
    "            agent = cur_agents_pursuers[pursuer_id]\n",
    "            state_tensor = preprocess_observation(states_dict_numpy[pursuer_id], training_device)\n",
    "            action_tensor = agent.select_action(state_tensor)\n",
    "            actions_to_env[pursuer_id] = action_tensor.item()\n",
    "            current_pursuer_actions_for_memory[pursuer_id] = action_tensor\n",
    "\n",
    "        active_evaders_this_step = [eid for eid in evader_ids if eid in env.agents and eid in states_dict_numpy]\n",
    "        for evader_id in active_evaders_this_step:\n",
    "            actions_to_env[evader_id] = env.action_space(evader_id).sample()\n",
    "\n",
    "        if not env.agents: break  # All agents are done from PZ perspective\n",
    "\n",
    "        next_states_dict_numpy, rewards_dict, terminations_dict, truncations_dict, infos_dict = env.step(actions_to_env)\n",
    "\n",
    "        for pursuer_id in active_pursuers_this_step:\n",
    "            agent = cur_agents_pursuers[pursuer_id]\n",
    "            reward_val = rewards_dict.get(pursuer_id, 0.0)\n",
    "            reward_tensor = torch.tensor([[reward_val]], device=training_device, dtype=torch.float32)\n",
    "\n",
    "            current_state_tensor = preprocess_observation(states_dict_numpy[pursuer_id], training_device)\n",
    "            action_taken_tensor = current_pursuer_actions_for_memory[pursuer_id]\n",
    "\n",
    "            next_state_tensor_for_memory = None\n",
    "            agent_terminated = terminations_dict.get(pursuer_id, False)\n",
    "            agent_truncated = truncations_dict.get(pursuer_id, False)\n",
    "\n",
    "            if not (agent_terminated or agent_truncated):\n",
    "                if pursuer_id in next_states_dict_numpy:\n",
    "                    next_state_tensor_for_memory = preprocess_observation(next_states_dict_numpy[pursuer_id],\n",
    "                                                                          training_device)\n",
    "\n",
    "            agent.memorize(current_state_tensor, action_taken_tensor, next_state_tensor_for_memory, reward_tensor)\n",
    "\n",
    "            loss = agent.train()\n",
    "            if loss is not None:\n",
    "                episode_total_loss_pursuers[pursuer_id] += loss\n",
    "                episode_steps_with_loss_pursuers[pursuer_id] += 1\n",
    "\n",
    "        if not env.agents or t_step >= MAX_CYCLES - 1:  # PZ: env.agents is empty if all done\n",
    "            break\n",
    "        states_dict_numpy = next_states_dict_numpy\n",
    "\n",
    "    for pursuer_id in pursuer_ids:\n",
    "        cur_agents_pursuers[pursuer_id].update_eps()\n",
    "\n",
    "    avg_loss_strings = []\n",
    "    for p_id in pursuer_ids:\n",
    "        if episode_steps_with_loss_pursuers[p_id] > 0:\n",
    "            avg_loss = episode_total_loss_pursuers[p_id] / episode_steps_with_loss_pursuers[p_id]\n",
    "            avg_loss_strings.append(f\"{p_id.split('_')[-1]}: {avg_loss:.3f}\")\n",
    "        else:\n",
    "            avg_loss_strings.append(f\"{p_id.split('_')[-1]}: N/A\")\n",
    "    avg_episode_loss_str = \", \".join(avg_loss_strings)\n",
    "\n",
    "    print(\n",
    "        f\"Ep {i_episode + 1}/{N_TRAIN_EPISODES} ({t_step + 1} steps). Avg Loss: [{avg_episode_loss_str}] Eps: {cur_agents_pursuers[pursuer_ids[0]].eps:.3f}\")\n",
    "\n",
    "    if (i_episode + 1) % TARGET_NET_UPDATE_STRATEGY_FREQ == 0:\n",
    "        current_best_overall_reward = update_all_target_networks_if_better_new(\n",
    "            cur_agents_pursuers, current_best_overall_reward, eval_episodes=3, device_for_eval=training_device)\n",
    "\n",
    "    if (i_episode + 1) % 10 == 0 or i_episode == N_TRAIN_EPISODES - 1:\n",
    "        if N_TRAIN_EPISODES > 1:\n",
    "            print(f\"Plotting: Evaluating policy_nets for episode {i_episode + 1} performance...\")\n",
    "            current_perf_res = eval_pursuit_agents_new(cur_agents_pursuers, dqns_to_use_type=\"policy\", n_episodes=3,\n",
    "                                                       eval_device=training_device)\n",
    "            avg_current_perf_per_agent = get_agent_wise_cumulative_rewards(current_perf_res)\n",
    "            if avg_current_perf_per_agent:\n",
    "                current_overall_perf = sum(avg_current_perf_per_agent.values()) / len(avg_current_perf_per_agent)\n",
    "                episode_mean_rewards_history.append(current_overall_perf)\n",
    "                print(f\"Plotting: Current policy_net overall performance: {current_overall_perf:.3f}\")\n",
    "                if 'plt' in globals() and 'display' in globals() and is_ipython: plot_episodes_rewards(\n",
    "                    episode_mean_rewards_history)\n",
    "            else:\n",
    "                print(\"Plotting: No performance data to plot.\")\n",
    "\n",
    "if 'plt' in globals() and 'display' in globals() and is_ipython and episode_mean_rewards_history:\n",
    "    plt.ioff()\n",
    "    plot_episodes_rewards(episode_mean_rewards_history)\n",
    "    plt.show()\n",
    "elif 'plt' in globals() and plt:  # Check if plt was imported and figure exists\n",
    "    try:\n",
    "        if plt.get_fignums():  # Check if any figures are open\n",
    "            plt.close('all')\n",
    "    except Exception:  # Catch any errors if backend doesn't support get_fignums well\n",
    "        pass\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# %% md\n",
    "# ## Final Evaluation\n",
    "# %%\n",
    "print(\"\\nEvaluating trained agents (using final target_net)...\")\n",
    "final_eval_res = eval_pursuit_agents_new(cur_agents_pursuers, dqns_to_use_type=\"target\", n_episodes=10,\n",
    "                                         eval_device=training_device)\n",
    "avg_final_res = get_agent_wise_cumulative_rewards(final_eval_res)\n",
    "if avg_final_res:\n",
    "    print(\"> Average cumulative rewards (trained) per pursuer:\")\n",
    "    pprint(avg_final_res)\n",
    "    all_avg_final_res = sum(avg_final_res.values()) / len(avg_final_res)\n",
    "    print(f\"> Overall average cumulative reward (trained): {all_avg_final_res:.2f}\")\n",
    "else:\n",
    "    print(\"> No final evaluation results.\")\n",
    "\n",
    "# %% md\n",
    "# ## Render a few episodes with trained agents\n",
    "# %%\n",
    "# Ensure ENV_RENDER_MODE was None during training to avoid issues with multiple envs trying to render to same display\n",
    "# For rendering, it's cleaner to explicitly set render_mode=\"human\" in the eval call.\n",
    "if ENV_RENDER_MODE is None:\n",
    "    print(\"\\nRendering a few episodes with trained agents (target_net)...\")\n",
    "    eval_pursuit_agents_new(cur_agents_pursuers, dqns_to_use_type=\"target\", n_episodes=3, render=True,\n",
    "                            eval_device=training_device)\n",
    "\n",
    "env.close()  # Close the main training environment instance\n",
    "print(\"Demo finished.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pursuers: ['pursuer_0', 'pursuer_1', 'pursuer_2', 'pursuer_3', 'pursuer_4', 'pursuer_5', 'pursuer_6', 'pursuer_7']\n",
      "Evaders: []\n",
      "Pursuer Obs Dim (flat): 147\n",
      "Pursuer Act Dim: 5\n",
      "Using device: cpu\n",
      "Initialized 8 pursuer agents.\n",
      "Evaluating untrained agents (using policy_net, which is same as target_net initially)...\n",
      "> Average cumulative rewards (baseline) per pursuer:\n",
      "{'pursuer_0': np.float64(-19.78781250000001),\n",
      " 'pursuer_1': np.float64(-19.78781250000001),\n",
      " 'pursuer_2': np.float64(-19.78781250000001),\n",
      " 'pursuer_3': np.float64(-19.78781250000001),\n",
      " 'pursuer_4': np.float64(-19.78781250000001),\n",
      " 'pursuer_5': np.float64(-19.78781250000001),\n",
      " 'pursuer_6': np.float64(-19.78781250000001),\n",
      " 'pursuer_7': np.float64(-19.78781250000001)}\n",
      "> Overall average cumulative reward (baseline): -19.79\n",
      "\n",
      "Starting training for 100 episodes...\n",
      "Ep 1/100 (200 steps). Avg Loss: [0: 0.033, 1: 0.053, 2: 0.050, 3: 0.026, 4: 0.035, 5: 0.022, 6: 0.052, 7: 0.088] Eps: 0.999\n",
      "Ep 2/100 (200 steps). Avg Loss: [0: 0.009, 1: 0.013, 2: 0.011, 3: 0.010, 4: 0.011, 5: 0.007, 6: 0.012, 7: 0.012] Eps: 0.998\n",
      "Ep 3/100 (200 steps). Avg Loss: [0: 0.007, 1: 0.009, 2: 0.005, 3: 0.007, 4: 0.010, 5: 0.004, 6: 0.005, 7: 0.009] Eps: 0.997\n",
      "Ep 4/100 (200 steps). Avg Loss: [0: 0.007, 1: 0.007, 2: 0.006, 3: 0.006, 4: 0.008, 5: 0.004, 6: 0.006, 7: 0.009] Eps: 0.996\n",
      "Ep 5/100 (200 steps). Avg Loss: [0: 0.005, 1: 0.006, 2: 0.004, 3: 0.006, 4: 0.007, 5: 0.004, 6: 0.005, 7: 0.008] Eps: 0.995\n",
      "Ep 6/100 (200 steps). Avg Loss: [0: 0.006, 1: 0.007, 2: 0.005, 3: 0.006, 4: 0.007, 5: 0.004, 6: 0.005, 7: 0.009] Eps: 0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
