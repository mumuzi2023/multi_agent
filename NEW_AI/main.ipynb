{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MARL with IQL (CNN) - Pursuit Environment\n",
    "# Independent Q-Learning adapted for the PettingZoo SISL Pursuit environment.\n",
    "# This version uses a CNN for processing image-like observations from Pursuit.\n",
    "# Basic imports\n",
    "import torch\n",
    "import torch.nn as nn  # Though not directly used here, agent.py and dqn.py use it\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict  # For CqlAgentConfig if used, not primary here\n",
    "from itertools import count\n",
    "from pprint import pprint\n",
    "import gymnasium as gym  # PettingZoo uses Gymnasium spaces\n",
    "# Environment and Agent imports\n",
    "from pettingzoo.sisl import pursuit_v4  # MODIFIED: Import Pursuit environment\n",
    "from agent import DqnAgent, DqnAgentConfig  # MODIFIED: Assuming agent.py is updated\n",
    "# DQN_CNN and DQN_MLP are imported within agent.py from dqn.py\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# Determine device\n",
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if torch.backends.mps.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Environment Setup\n",
    "# MODIFIED: Environment parameters for Pursuit\n",
    "# These are example parameters, tune as needed.\n",
    "MAX_CYCLES_ENV = 500\n",
    "N_EVADERS = 5\n",
    "N_PURSUERS = 4  # Number of agents we will control\n",
    "OBS_RANGE = 7  # Affects observation HxW; H=W=OBS_RANGE\n",
    "N_CATCH = 2  # Number of pursuers needed to catch an evader\n",
    "\n",
    "# MODIFIED: Instantiate Pursuit environment\n",
    "env = pursuit_v4.parallel_env(\n",
    "    max_cycles=MAX_CYCLES_ENV,\n",
    "    n_evaders=N_EVADERS,\n",
    "    n_pursuers=N_PURSUERS,\n",
    "    obs_range=OBS_RANGE,\n",
    "    n_catch=N_CATCH,\n",
    "    render_mode='rgb_array'  # For potential rendering/observation consistency\n",
    ")\n",
    "env.reset()  # Call reset once to initialize agents list\n",
    "\n",
    "# MODIFIED: Identify agents to be controlled (pursuers)\n",
    "# In pursuit_v4, agents are named e.g., \"pursuer_0\", \"evader_0\"\n",
    "ALL_AGENT_IDS_FROM_ENV = env.agents\n",
    "CONTROLLED_AGENT_KEYS = sorted([agent_id for agent_id in ALL_AGENT_IDS_FROM_ENV if \"pursuer\" in agent_id])\n",
    "\n",
    "if not CONTROLLED_AGENT_KEYS:\n",
    "    raise ValueError(\"No pursuers found to control. Check N_PURSUERS or agent naming in the environment.\")\n",
    "num_controlled_agents = len(CONTROLLED_AGENT_KEYS)\n",
    "print(f\"Controlling {num_controlled_agents} pursuer(s): {CONTROLLED_AGENT_KEYS}\")\n",
    "\n",
    "# MODIFIED: Extract observation shapes and action dimensions for controlled agents\n",
    "# Pursuit observations are typically image-like: Box(0, 255, (H, W, C), np.uint8)\n",
    "# We need to convert to PyTorch's NCHW format. Config will store CHW.\n",
    "obs_shapes = {}  # Stores (C, H, W)\n",
    "act_dims = {}\n",
    "\n",
    "for agent_key in CONTROLLED_AGENT_KEYS:\n",
    "    obs_space = env.observation_space(agent_key)\n",
    "    act_space = env.action_space(agent_key)\n",
    "\n",
    "    if not isinstance(obs_space, gym.spaces.Box) or len(obs_space.shape) != 3:\n",
    "        raise ValueError(f\"Expected Box observation space with 3 dims (H,W,C) for agent {agent_key}, got {obs_space}\")\n",
    "\n",
    "    h, w, c = obs_space.shape  # Original HWC\n",
    "    obs_shapes[agent_key] = (c, h, w)  # Store as CHW for PyTorch\n",
    "    act_dims[agent_key] = int(act_space.n)\n",
    "\n",
    "print(f\"Observation Shapes (C, H, W) for controlled agents: {obs_shapes}\")\n",
    "print(f\"Action Dimensions for controlled agents: {act_dims}\")\n",
    "\n",
    "# Example observation check\n",
    "# temp_states_dict, _ = env.reset()\n",
    "# if CONTROLLED_AGENT_KEYS:\n",
    "#     example_obs_raw = temp_states_dict[CONTROLLED_AGENT_KEYS[0]]\n",
    "#     print(f\"Example raw observation for {CONTROLLED_AGENT_KEYS[0]} - shape: {example_obs_raw.shape}, dtype: {example_obs_raw.dtype}\")\n",
    "# del temp_states_dict, example_obs_raw\n",
    "\n",
    "# Agent Initialization\n",
    "USE_CNN = True  # MODIFIED: Set to True to use DQN_CNN\n",
    "\n",
    "# MODIFIED: Agent Configuration (tune these hyperparameters)\n",
    "AGENT_CONFIGS = {}\n",
    "for agent_key in CONTROLLED_AGENT_KEYS:\n",
    "    AGENT_CONFIGS[agent_key] = DqnAgentConfig(\n",
    "        obs_shape=obs_shapes[agent_key] if USE_CNN else None,  # Pass CHW shape\n",
    "        obs_dim=int(np.prod(obs_shapes[agent_key])) if not USE_CNN else None,  # Pass flattened size if MLP\n",
    "        act_dim=act_dims[agent_key],\n",
    "        hidden_dim=256,  # Num units in FC layer after CNN, or hidden_dim for MLP\n",
    "        batch_size=32,  # Might need adjustment based on memory\n",
    "        lr=1e-4,  # CNNs often benefit from smaller LRs\n",
    "        grad_clip_value=1.0,  # Common for DQN\n",
    "        gamma=0.99,\n",
    "        eps_start=1.0,\n",
    "        eps_decay=0.9995,  # Slower decay for more complex tasks\n",
    "        eps_min=0.05,\n",
    "        mem_size=50000,  # Replay memory size\n",
    "        use_cnn=USE_CNN\n",
    "    )\n",
    "\n",
    "# Create agent instances\n",
    "cur_agents = {\n",
    "    agent_key: DqnAgent(\n",
    "        sid=agent_key,\n",
    "        config=AGENT_CONFIGS[agent_key],\n",
    "        act_sampler=env.action_space(agent_key).sample,  # Pass the sampler\n",
    "        device=DEVICE\n",
    "    )\n",
    "    for agent_key in CONTROLLED_AGENT_KEYS\n",
    "}\n",
    "\n",
    "if cur_agents:\n",
    "    print(f\"Created {len(cur_agents)} DqnAgent(s). First agent's policy network:\")\n",
    "    print(list(cur_agents.values())[0].policy_net)\n",
    "else:\n",
    "    print(\"No agents were created.\")\n",
    "\n",
    "\n",
    "# Helper Function for State Preprocessing (for CNN)\n",
    "def preprocess_observation(obs_hwc: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Converts HWC observation to NCHW PyTorch tensor and normalizes.\"\"\"\n",
    "    if obs_hwc is None:  # Should not happen if agent is active\n",
    "        raise ValueError(\"Received None observation for an active agent.\")\n",
    "    # Normalize to [0, 1]\n",
    "    obs_normalized = np.array(obs_hwc, dtype=np.float32) / 255.0\n",
    "    # Permute HWC to CHW and add batch dimension N=1\n",
    "    obs_tensor_chw = torch.tensor(obs_normalized, device=device).permute(2, 0, 1)\n",
    "    return obs_tensor_chw.unsqueeze(0)  # NCHW\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "# Assuming gym is imported if needed by PettingZoo, or import directly\n",
    "\n",
    "\n",
    "\n",
    "def eval_agent(\n",
    "        eval_env_lambda: Callable,  # Lambda function to create a new eval environment\n",
    "        dqn_agents_dict: dict[str, DqnAgent],\n",
    "        networks_to_use: dict[str, nn.Module],  # agent_key -> network (e.g., policy_net or target_net)\n",
    "        n_episodes: int = 1,\n",
    "        render_mode_eval: str | None = None  # 'human' or 'rgb_array'\n",
    ") -> dict[str, list[float]]:\n",
    "    cumulative_rewards_per_agent = {agent_key: [] for agent_key in dqn_agents_dict.keys()}\n",
    "    eval_device = list(dqn_agents_dict.values())[0].device  # Get device from first agent\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        eval_env = eval_env_lambda()  # Create a fresh environment\n",
    "        if render_mode_eval:  # Set render mode for this specific eval env instance\n",
    "            # This might not work for all PettingZoo envs if render_mode is set at init\n",
    "            # For pursuit_v4, render_mode is set at init. We'll rely on that.\n",
    "            # If you want to change it dynamically, env might need a set_render_mode method.\n",
    "            pass\n",
    "\n",
    "        raw_states_dict, _ = eval_env.reset()\n",
    "\n",
    "        # MODIFIED: Preprocess initial states for controlled agents\n",
    "        current_states_processed = {\n",
    "            agent_key: preprocess_observation(raw_states_dict[agent_key], eval_device)\n",
    "            for agent_key in dqn_agents_dict.keys() if agent_key in raw_states_dict\n",
    "        }\n",
    "\n",
    "        # Dones only for controlled agents\n",
    "        episode_dones = {agent_key: False for agent_key in dqn_agents_dict.keys()}\n",
    "        episode_cumulative_rewards = {agent_key: 0.0 for agent_key in dqn_agents_dict.keys()}\n",
    "\n",
    "        for t in count():\n",
    "            if render_mode_eval == 'human':\n",
    "                eval_env.render()  # Render if in human mode\n",
    "\n",
    "            actions_to_env = {}\n",
    "            for agent_key, agent_instance in dqn_agents_dict.items():\n",
    "                if agent_key in current_states_processed and not episode_dones[agent_key]:\n",
    "                    # Use greedy action selection with the specified network\n",
    "                    action_tensor = agent_instance.select_action_greedy(\n",
    "                        current_states_processed[agent_key],\n",
    "                        networks_to_use[agent_key]\n",
    "                    )\n",
    "                    actions_to_env[agent_key] = action_tensor.item()\n",
    "\n",
    "            if not actions_to_env:  # All controlled agents are done\n",
    "                break\n",
    "\n",
    "            raw_next_obs_dict, raw_rewards_dict, raw_terminations_dict, raw_truncations_dict, _ = eval_env.step(\n",
    "                actions_to_env)\n",
    "\n",
    "            next_states_processed = {}\n",
    "            for agent_key in dqn_agents_dict.keys():\n",
    "                if episode_dones[agent_key]:  # If agent was already done, skip\n",
    "                    if agent_key in current_states_processed:  # Keep its last state if needed, or None\n",
    "                        next_states_processed[agent_key] = current_states_processed[agent_key]\n",
    "                    continue\n",
    "\n",
    "                # Update rewards and dones for controlled agents\n",
    "                episode_cumulative_rewards[agent_key] += raw_rewards_dict.get(agent_key, 0)\n",
    "\n",
    "                terminated = raw_terminations_dict.get(agent_key, False)\n",
    "                truncated = raw_truncations_dict.get(agent_key, False)\n",
    "                episode_dones[agent_key] = terminated or truncated\n",
    "\n",
    "                if not episode_dones[agent_key] and agent_key in raw_next_obs_dict:\n",
    "                    next_states_processed[agent_key] = preprocess_observation(raw_next_obs_dict[agent_key], eval_device)\n",
    "                elif agent_key in current_states_processed:  # If done, can reuse last state or set to None\n",
    "                    next_states_processed[agent_key] = current_states_processed[\n",
    "                        agent_key]  # or None, if network handles it\n",
    "\n",
    "            current_states_processed = next_states_processed\n",
    "\n",
    "            if all(episode_dones.values()):\n",
    "                break\n",
    "\n",
    "        if render_mode_eval == 'human':\n",
    "            eval_env.close()  # Close env if rendered explicitly, not strictly needed for parallel_env auto-reset\n",
    "\n",
    "        for agent_key in dqn_agents_dict.keys():\n",
    "            cumulative_rewards_per_agent[agent_key].append(episode_cumulative_rewards[agent_key])\n",
    "\n",
    "    return cumulative_rewards_per_agent\n",
    "\n",
    "\n",
    "def get_agent_wise_cumulative_rewards(cumulative_rewards_dict: dict[str, list[float]]) -> dict[str, float]:\n",
    "    return {\n",
    "        agent_key: sum(rewards_list) / len(rewards_list) if rewards_list else 0.0\n",
    "        for agent_key, rewards_list in cumulative_rewards_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "# Baseline Evaluation (Before Training)\n",
    "if cur_agents:\n",
    "    print(\"Running baseline evaluation...\")\n",
    "    # Lambda to create pursuit environment for evaluation\n",
    "    eval_env_lambda_pursuit = lambda: pursuit_v4.parallel_env(\n",
    "        max_cycles=MAX_CYCLES_ENV, n_evaders=N_EVADERS, n_pursuers=num_controlled_agents,\n",
    "        obs_range=OBS_RANGE, n_catch=N_CATCH, render_mode='rgb_array'  # Eval does not render human by default\n",
    "    )\n",
    "\n",
    "    baseline_eval_res = eval_agent(\n",
    "        eval_env_lambda_pursuit,\n",
    "        cur_agents,\n",
    "        dqns={agent.sid: agent.target_net for agent in cur_agents.values()},  # Use target_net for stable eval\n",
    "        n_episodes=10  # Number of episodes for baseline evaluation\n",
    "    )\n",
    "    avg_baseline_res = get_agent_wise_cumulative_rewards(baseline_eval_res)\n",
    "    pprint(f\"Average baseline rewards per agent: {avg_baseline_res}\")\n",
    "    all_avg_baseline_res = sum(avg_baseline_res.values()) / len(avg_baseline_res) if avg_baseline_res else 0.0\n",
    "    print(f\"Overall average baseline reward: {all_avg_baseline_res}\")\n",
    "else:\n",
    "    print(\"No agents to evaluate for baseline.\")\n",
    "    all_avg_baseline_res = float('-inf')\n",
    "\n",
    "\n",
    "# Plotting Module\n",
    "def plot_episodes(avg_returns: list[float], title_suffix: str = ''):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    returns_t = torch.tensor(avg_returns, dtype=torch.float)\n",
    "    plt.title(f'Training... {title_suffix}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg. Return (across all controlled agents)')\n",
    "    plt.plot(returns_t.numpy())\n",
    "\n",
    "    if len(returns_t) >= 10:  # Plot 10-episode rolling mean\n",
    "        means = returns_t.unfold(0, 10, 1).mean(1).reshape(-1)\n",
    "        means = torch.cat((torch.full((9,), float('nan')), means))  # Pad with NaNs for alignment\n",
    "        plt.plot(means.numpy(), label='10-ep Avg.')\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Training Setup\n",
    "N_TRAIN_EPISODES = 500  # MODIFIED: Number of training episodes\n",
    "MAX_TRAIN_EPISODE_STEPS = MAX_CYCLES_ENV  # Max steps per episode, from env\n",
    "DQN_UPDATE_TARGET_FREQ_STEPS = 200  # MODIFIED: How often (in total steps) to consider updating target net\n",
    "\n",
    "# Lambda for creating training environment\n",
    "train_env_lambda = lambda: pursuit_v4.parallel_env(\n",
    "    max_cycles=MAX_CYCLES_ENV, n_evaders=N_EVADERS, n_pursuers=num_controlled_agents,\n",
    "    obs_range=OBS_RANGE, n_catch=N_CATCH, render_mode='rgb_array'\n",
    ")\n",
    "# Re-initialize main training env using the lambda for consistency\n",
    "env = train_env_lambda()\n",
    "\n",
    "\n",
    "# Target Network Update Logic\n",
    "def update_all_agents_target_dqns(\n",
    "        current_agents: dict[str, DqnAgent],\n",
    "        eval_env_fn: Callable,\n",
    "        current_best_mean_reward: float,\n",
    "        n_eval_episodes: int = 5\n",
    ") -> float:\n",
    "    \"\"\"Evaluates policy nets and updates target nets if performance improved.\"\"\"\n",
    "    print(\"Evaluating policy networks for potential target network update...\")\n",
    "    policy_eval_res = eval_agent(\n",
    "        eval_env_fn,\n",
    "        current_agents,\n",
    "        dqns={agent.sid: agent.policy_net for agent in current_agents.values()},\n",
    "        n_episodes=n_eval_episodes\n",
    "    )\n",
    "    avg_policy_rewards = get_agent_wise_cumulative_rewards(policy_eval_res)\n",
    "    overall_avg_policy_reward = sum(avg_policy_rewards.values()) / len(\n",
    "        avg_policy_rewards) if avg_policy_rewards else float('-inf')\n",
    "\n",
    "    print(\n",
    "        f\"Policy net eval: current avg reward {overall_avg_policy_reward:.2f} vs best mean {current_best_mean_reward:.2f}\")\n",
    "    if overall_avg_policy_reward > current_best_mean_reward:\n",
    "        print(f\"Improvement found! Updating target networks. New best mean: {overall_avg_policy_reward:.2f}\")\n",
    "        current_best_mean_reward = overall_avg_policy_reward\n",
    "        for agent in current_agents.values():\n",
    "            agent.update_target_network()\n",
    "    else:\n",
    "        print(\"No improvement, target networks not updated.\")\n",
    "    return current_best_mean_reward\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "if not cur_agents:\n",
    "    print(\"No agents defined. Skipping training.\")\n",
    "else:\n",
    "    total_steps_done = 0\n",
    "    best_overall_mean_reward = all_avg_baseline_res  # Initialize with baseline\n",
    "    episode_mean_returns_log = [all_avg_baseline_res] if all_avg_baseline_res != float('-inf') else []\n",
    "\n",
    "    print(f\"Starting training for {N_TRAIN_EPISODES} episodes...\")\n",
    "    for i_episode in range(N_TRAIN_EPISODES):\n",
    "        raw_states_dict, _ = env.reset()\n",
    "\n",
    "        current_states_processed = {\n",
    "            agent_key: preprocess_observation(raw_states_dict[agent_key], DEVICE)\n",
    "            for agent_key in CONTROLLED_AGENT_KEYS if agent_key in raw_states_dict\n",
    "        }\n",
    "\n",
    "        # Dones for controlled agents for the current episode\n",
    "        episode_dones = {agent_key: False for agent_key in CONTROLLED_AGENT_KEYS}\n",
    "        episode_total_reward_for_plot = 0\n",
    "        num_active_agents_for_plot = len(CONTROLLED_AGENT_KEYS)\n",
    "\n",
    "        for t_step in range(MAX_TRAIN_EPISODE_STEPS):\n",
    "            actions_for_env_step = {}\n",
    "            current_actions_this_step = {}  # To store action tensors for memory\n",
    "\n",
    "            for agent_key in CONTROLLED_AGENT_KEYS:\n",
    "                if not episode_dones[agent_key] and agent_key in current_states_processed:\n",
    "                    agent = cur_agents[agent_key]\n",
    "                    action_tensor = agent.select_action(current_states_processed[agent_key])\n",
    "                    actions_for_env_step[agent_key] = action_tensor.item()\n",
    "                    current_actions_this_step[agent_key] = action_tensor\n",
    "\n",
    "            if not actions_for_env_step:  # All controlled agents are done\n",
    "                break\n",
    "\n",
    "            raw_next_obs_dict, raw_rewards_dict, raw_terminations_dict, raw_truncations_dict, _ = env.step(\n",
    "                actions_for_env_step)\n",
    "\n",
    "            next_states_processed_this_step = {}\n",
    "\n",
    "            for agent_key in CONTROLLED_AGENT_KEYS:\n",
    "                agent = cur_agents[agent_key]\n",
    "\n",
    "                # If agent was already done, its state for memory is its last known state, next_state is None\n",
    "                if episode_dones[agent_key]:\n",
    "                    if agent_key in current_actions_this_step:  # Only memorize if it took an action\n",
    "                        agent.memorize(\n",
    "                            current_states_processed[agent_key],\n",
    "                            current_actions_this_step[agent_key],\n",
    "                            None,  # Terminal state\n",
    "                            torch.tensor([[raw_rewards_dict.get(agent_key, 0)]], device=DEVICE, dtype=torch.float32)\n",
    "                        )\n",
    "                    continue  # Skip further processing for this already-done agent\n",
    "\n",
    "                # Process current step's outcome for active agents\n",
    "                reward_val = raw_rewards_dict.get(agent_key, 0)\n",
    "                episode_total_reward_for_plot += reward_val  # Summing rewards for plotting average later\n",
    "\n",
    "                terminated = raw_terminations_dict.get(agent_key, False)\n",
    "                truncated = raw_truncations_dict.get(agent_key, False)\n",
    "                is_done_this_step = terminated or truncated\n",
    "\n",
    "                next_state_for_memory = None\n",
    "                if not is_done_this_step and agent_key in raw_next_obs_dict:\n",
    "                    processed_next_obs = preprocess_observation(raw_next_obs_dict[agent_key], DEVICE)\n",
    "                    next_states_processed_this_step[agent_key] = processed_next_obs\n",
    "                    next_state_for_memory = processed_next_obs\n",
    "\n",
    "                # Memorize, only if an action was taken by this agent this step\n",
    "                if agent_key in current_actions_this_step:\n",
    "                    agent.memorize(\n",
    "                        current_states_processed[agent_key],\n",
    "                        current_actions_this_step[agent_key],\n",
    "                        next_state_for_memory,\n",
    "                        torch.tensor([[reward_val]], device=DEVICE, dtype=torch.float32)\n",
    "                    )\n",
    "\n",
    "                agent.train()  # Try to train agent\n",
    "                episode_dones[agent_key] = is_done_this_step  # Update done status\n",
    "\n",
    "            current_states_processed = next_states_processed_this_step  # Move to next state\n",
    "\n",
    "            # Update target network periodically\n",
    "            if total_steps_done % DQN_UPDATE_TARGET_FREQ_STEPS == 0 and total_steps_done > 0:\n",
    "                best_overall_mean_reward = update_all_agents_target_dqns(\n",
    "                    cur_agents, train_env_lambda, best_overall_mean_reward\n",
    "                )\n",
    "\n",
    "            for agent in cur_agents.values():  # Epsilon decay for all controlled agents\n",
    "                agent.update_eps()\n",
    "\n",
    "            total_steps_done += 1\n",
    "\n",
    "            if all(episode_dones.values()):  # If all controlled agents are done\n",
    "                break\n",
    "\n",
    "        # End of episode actions\n",
    "        # Always try to update target networks at the end of an episode based on policy net performance\n",
    "        best_overall_mean_reward = update_all_agents_target_dqns(\n",
    "            cur_agents, train_env_lambda, best_overall_mean_reward\n",
    "        )\n",
    "\n",
    "        # Log episode return for plotting (average across controlled agents)\n",
    "        avg_episode_return = episode_total_reward_for_plot / num_controlled_agents if num_controlled_agents > 0 else 0\n",
    "        episode_mean_returns_log.append(avg_episode_return)\n",
    "        plot_episodes(episode_mean_returns_log)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {i_episode + 1}/{N_TRAIN_EPISODES} finished. Avg Return: {avg_episode_return:.2f}. Epsilon (agent 0): {cur_agents[CONTROLLED_AGENT_KEYS[0]].eps:.3f}. Total steps: {total_steps_done}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    plot_episodes(episode_mean_returns_log, title_suffix=\" - Final\")  # Final plot\n",
    "    plt.ioff()  # Turn off interactive plotting\n",
    "    plt.show()\n",
    "\n",
    "# Post-Training Evaluation\n",
    "if cur_agents:\n",
    "    print(\"Running post-training evaluation...\")\n",
    "    post_train_eval_res = eval_agent(\n",
    "        train_env_lambda,  # Use the same lambda as training for consistency\n",
    "        cur_agents,\n",
    "        dqns={agent.sid: agent.target_net for agent in cur_agents.values()},  # Evaluate final target_net\n",
    "        n_episodes=20\n",
    "    )\n",
    "    avg_post_train_res = get_agent_wise_cumulative_rewards(post_train_eval_res)\n",
    "    pprint(f\"Average post-training rewards per agent: {avg_post_train_res}\")\n",
    "    all_avg_post_train_res = sum(avg_post_train_res.values()) / len(avg_post_train_res) if avg_post_train_res else 0.0\n",
    "    print(f\"Overall average post-training reward: {all_avg_post_train_res}\")\n",
    "else:\n",
    "    print(\"No agents to evaluate post-training.\")\n",
    "\n",
    "# Rendered Evaluation (Optional)\n",
    "# if cur_agents:\n",
    "#     print(\"Running rendered evaluation...\")\n",
    "#     # For rendered eval, create an env with 'human' mode if supported and desired\n",
    "#     rendered_eval_env_lambda = lambda: pursuit_v4.parallel_env(\n",
    "#         max_cycles=MAX_CYCLES_ENV, n_evaders=N_EVADERS, n_pursuers=num_controlled_agents,\n",
    "#         obs_range=OBS_RANGE, n_catch=N_CATCH, render_mode='human' # Set to human for visual\n",
    "#     )\n",
    "#     eval_agent(\n",
    "#         rendered_eval_env_lambda,\n",
    "#         cur_agents,\n",
    "#         dqns={agent.sid: agent.target_net for agent in cur_agents.values()},\n",
    "#         n_episodes=1,\n",
    "#         render_mode_eval='human' # Pass render mode to eval_agent\n",
    "#     )\n",
    "#     print(\"Rendered evaluation finished. (Close the window if it opened)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
